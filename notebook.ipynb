{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "A notebook to ask GPT-3.5-Turbo questions about a PDF document while keeping track of the previous questions (memory).\n",
        "\n",
        "<a href=\"https://colab.research.google.com/gist/blekmus/89818776e181cf28dfd09968c419521b/gpt3-5-with-memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1dPEJ72oFIn"
      },
      "outputs": [],
      "source": [
        "# install dependancies\n",
        "!pip install langchain openai pypdf tiktoken chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-FWn16Pus0l"
      },
      "outputs": [],
      "source": [
        "# download pdf\n",
        "!curl -o document.pdf \"https://bitcoin.org/bitcoin.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NJPd6ArJ-5v"
      },
      "outputs": [],
      "source": [
        "openai_api_key = \"sk-something\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZTI5brfrXt3"
      },
      "outputs": [],
      "source": [
        "# prepare pdf\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "def tiktoken_len(text):\n",
        "    tokens = tokenizer.encode(text, disallowed_special=())\n",
        "    return len(tokens)\n",
        "\n",
        "pdf_loader = PyPDFLoader(\"./document.pdf\")\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=tiktoken_len,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        ")\n",
        "\n",
        "pdf_data = pdf_loader.load_and_split(text_splitter=text_splitter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9ss9rNNyvO6",
        "outputId": "8549c1dc-505c-4cb3-f729-4a0ffbaddc14"
      },
      "outputs": [],
      "source": [
        "# create embedding\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    pdf_data, \n",
        "    embedding=embeddings, \n",
        "    persist_directory=\".\"\n",
        ")\n",
        "\n",
        "vectordb.persist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "MPRGVGqsLDjP",
        "outputId": "a2e7bf5a-8757-41d4-995a-3c2d127a0501"
      },
      "outputs": [],
      "source": [
        "# initialize chat\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0, openai_api_key=openai_api_key)\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "conversation = ConversationalRetrievalChain.from_llm(\n",
        "    llm, \n",
        "    retriever=vectordb.as_retriever(), \n",
        "    memory=memory,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# chat\n",
        "chat_history = []\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"> \")\n",
        "    ai_response = conversation({\n",
        "        \"question\": user_input,\n",
        "        \"chat_history\": chat_history\n",
        "    })\n",
        "    print(ai_response['answer'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNRmj4fthuLKfgs73YDQtXJ",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
